---
Title: "Enron Corpus Text Analytics"
Author: "Srisai Sivakumar"
Date: "Tuesday, July 21, 2015"
Output: html_document
---

# Enron Fraud: Email/Text Analytics

## By: Srisai Sivakumar

Previously we saw the use of Analytics and Machine Learning to predict the judgements of cases by the US Supreme Court. In this section, we would still be discussing a problem that has legal dimension to it, but from a different context. In this study, we confine ourself to only the tree based models

### Introduction

Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. Before its bankruptcy on December 2, 2001, Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications, and pulp and paper companies, with claimed revenues of nearly $111 billion during 2000. Fortune named Enron "America's Most Innovative Company" for six consecutive years.

At the end of 2001, it was revealed that its reported financial condition was sustained substantially by an institutionalized, systematic, and creatively planned accounting fraud, known since as the Enron scandal. Enron has since become a well-known example of willful corporate fraud and corruption. 


### California Energy Crisis

The California electricity crisis, also known as the Western U.S. Energy Crisis of 2000 and 2001, was a situation in which the United States state of California had a shortage of electricity supply caused by market manipulations, illegal shutdowns of pipelines by the Texas energy consortium Enron, and capped retail electricity prices. The state suffered from multiple large-scale blackouts.

California had an installed generating capacity of 45GW. At the time of the blackouts, demand was 28GW. A demand supply gap was created by energy companies, mainly Enron, to create an artificial shortage. Energy traders took power plants offline for maintenance in days of peak demand to increase the price. Traders were thus able to sell power at premium prices, sometimes up to a factor of 20 times its normal value, thus makinf profit from the market instability.

The Federal Energy Regulatory Commission (FERC) investigated Enron's involvement. The investigation led to a $1.52 billion settlement with a group of California agencies and private utilities on July 16, 2005. However, due to its other bankruptcy obligations, only US$202 million of this was expected to be paid.


### The dataset

As a company of Enron's (then) stature, it had millions of electronic files. In this study, we will analyze the so called Enron Corpus. The Enron Corpus is a large database of over 600,000 emails generated by 158 employees of the Enron Corporation. The corpus is "unique" in that it is one of the only publicly available mass collections of "real" emails easily available for study, as such collections are typically bound by numerous privacy and legal restrictions which render them prohibitively difficult to access.


FERC publicly released emails from Enron. It had over 600,000 emails from 158 users, which consisted mostly of senior management officials. We will use labeled emails from the 2010 Text Retrieval Conference Legal Track. It consists of: 

- email - text of the message
- responsive - does email relate to energy schedules or bids?

The 'responsive' option bears the opinions of legal experts. We would use this model to predict if a selected email is likely to indicative of involvement in the bidding.

The set consists of around 860 emails, which after cleaning and transforming, would be split into the training and test sets.

### Approach

We will use Text Analytics, which according to Wikipedia "describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation".

It has to be kept in mind that Text Analytics comes with its own set of caveats. Texts such as emails and tweets are often 'loosely' structured. Its almost certain that 2 different individuals would structure their emails or tweets differently. And we find that such texts tend to have poor spellings, non-traditional grammar and at times, multi-lingual. I used to work for a Danish organization and I can vouch for the number of emails that had Danish content in it :)

Our text analytics process would involve exploring the corpus or body of text, cleaning it to be fit for analysis, transforming the text data into a 'sparse' matrix like data using technique such as 'Bag of Words', Stop Words, Stemming, etc., clustering the transformed data to visually inspect the patterns in it, other visual ways of exploring the data and analytically modelling the corpus to predict certain outcomes. 

### Analysis

#### The data

We begin by loading the data and inspecting the structure of the data.

```{r setup,echo=FALSE,message=FALSE,warning=FALSE}
setwd("C:/Users/S.Srisai/Documents/working_directory/R/AE/Text")
emails = read.csv("energy_bids.csv", stringsAsFactors=FALSE)
str(emails)
```

Lets take a look at the first 2 emails. Lets also check if these emails are responsive.

```{r emails,message=FALSE,warning=FALSE}
# display first email

emails$email[1]

# The display style is not easy to read. So lets wap the text (similar as text warp in excel, to fir the contents into one cell,, screen in our case)
strwrap(emails$email[1])

# responsive?
emails$responsive[1]

# Lets do it for the second email too.

emails$email[2]

emails$responsive[2]

# To get an understanding of the total number of responsive emails, lets to a table of values for the responsive feature of the data frame.

table(emails$responsive)


```

The secome email has turned out to be anti-climatic. Its a forwarded email with no inherent content in it. Such emails are beyond the scope of this study.

#### Transformations

We now begin the use of the 'tm' package.

We begin by cleaning and tidying the data. This involvs multiple steps.

It is important to know R treats 'help', 'Help', 'hElP', and 'HELP' differently. i.e. R treats upper and lower case representation of the same word as different words. So as part of the cleaning, we convert all the contents of the email to lower case.

First we define the data as a corpus and they do the lower case transformation. Once its done, we define the corpus to be a plain text document. We then proceed to remove parts of the text data that doesnt add any value to the analysis- like punctuations.


An example of a partly cleaned email is shown below.

```{r corpus_trans,echo=FALSE,warning=FALSE,message=FALSE}

library(tm)

corpus = Corpus(VectorSource(emails$email))

corpus[[1]]

corpus = tm_map(corpus, tolower)


# converting corpus to a Plain Text Document

corpus = tm_map(corpus, PlainTextDocument)

# removing punctuations

corpus = tm_map(corpus, removePunctuation)

# Look at first email

corpus[[1]][[1]]

```


##### Stop Words

Stop words are a set of commonly used words in any language, in our case, English. Stop words (its absence!) is important to many applications. It helps eliminate words that are unlikely to contribute to better predictionthus helps reduce the size of the data. It also helps the algorithm focus on the 'important' words in the context of the problem in hand. 

A simple analogy can be drawn using a google search. Say you want to learn R programming. You google 'how do I learn R programming'. It would give you lots of pages that match your search. But one must be mindful of the fact that it has search terms like 'how', 'do' and 'I', which do not contribute meaningfully over the search phrase 'leann r programming', but instead gives tou some pages that are relevant to the words 'I', 'do', etc. This is the basic idea of stop words. They can be used in a whole range of tasks and but we are using it for:

- Supervised machine learning: removing stop words from the feature space

- Clustering: removing stop words prior to generating clusters

Stop words can be thought to be a "single set of words". But in reality, mean different things to different applications. For example, in some applications removing all stop words right from determiners (e.g. the, a, an) to prepositions (e.g. above, across, before) to some adjectives (e.g. good, nice) can be an appropriate stop word list. This is expected to work well in our case. However to some applications, this can be detrimental. In sentiment analysis, for instance, removing adjective terms such as 'good' and 'nice' or 'not' can cause algorithms to misinterpret the data. In such cases, more thought needs to be given to the choice of Stop Words.


##### Stemming

Lets ask ourselves a question: Do we need to draw a distinction between the following words, in context of the task in hand? 

argue argued argues arguing
test tests testing

The answer is not necessarily. Distinction between these words are not expected to contribute to the performance of our prediction. it can be thought of a common 'stem' word like argu or test respectively can represent these words effectively for our analysis. The algorithmic process of performing this reduction is called stemming. Many ways to approach the problem.

One approach is to build a database of words and their stems

- Pro: handles exceptions

- Con: won't handle new words, bad for the Internet!

Another approach is developing a rule-based algorithm, where, for example, if word ends in "ed", "ing", or "ly", we remove it ending. 

- Pro: handles new/unknown words well

- Con: many exceptions, misses words like child and children (but would get other plurals: dog and dogs)


The second option is widely popular. The "Porter Stemmer" algorithm was developed by Martin Porter in the 80's, and is still widely used! We use the default stemming algorithm in the tm package.

An example of an email that has been removed of the stop words and stemmed is shown below.

```{r stop_stem,message=FALSE,warning=FALSE,echo=FALSE}

# stop words

corpus = tm_map(corpus, removeWords, stopwords("english"))

# stemming

corpus = tm_map(corpus, stemDocument)

#Lets look at the first email

corpus[[1]][[1]]

```


##### Sparsity

We not transform the corpus into a matrix and we proceed to make it sparse. The sparse matrix is, as the name suggests, is sparse, meaning that it has sparse data. Its mostly zeros. Processing such a matrix is easy and quick in terms of time and memory taken.

There needs to be a threshold for sparcity. In this study, we will consider a threshold of 3%. This means that all words that occur less than 3% of the times would be eliminated. This is a way of finding the vital few.


```{r sparce,message=FALSE,warning=FALSE,echo=FALSE}

dtm = DocumentTermMatrix(corpus)

# dtm

# Remove sparse terms

dtm = removeSparseTerms(dtm, 0.97)

dtm
```

##### Visualization of frequent words

Now that we have our final matrix needed for our analysis, lets explore it  to understand it better.

Lets look at the most and least frequently occuring words and see if it makes sense in this context.


```{r freq200,echo=FALSE,message=FALSE,warning=FALSE}
# find the frequently occuring words. lets set threshold of 200 and 400
freq.terms <- findFreqTerms(dtm, lowfreq = 200)
freq.terms
```


The above list shows the words that occur atleast 200 times in the data set. It does have a few words one would associate with an energy fraud endeavour. Lets increase the threshold to 400 and see if the words that occur atleast 400 times lends more insights into the scam.

```{r freq400,echo=FALSE,warning=FALSE,message=FALSE}
freq.terms <- findFreqTerms(dtm, lowfreq = 400)
freq.terms
```

The words does have terms one would associate with an energy company, but its hard to gain more insights from it.

Instead of setting arbitrary thresholds like 200 and 400 to measure word frequencies, lets find the most and least occurring words and display them. 

```{r freq1,echo=FALSE,message=FALSE,warning=FALSE}
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
# Least freqiently occuring words
freq[head(ord,20)]
# Most freqiently occuring words
freq[tail(ord,20)]
```

We see that there are 788 words in the list in total. The first table shows the least occurring words and the second, most occurring words.

Lets make some graphical representation of the word occurrences using ggplot.

```{r ggplot,echo=FALSE,warning=FALSE,message=FALSE}
# ggplot fo frequent words
tdm <- TermDocumentMatrix(corpus)
tt <- findFreqTerms(tdm, lowfreq=400)
termFrequency <- rowSums(as.matrix(tdm[tt,]))
library(ggplot2)
qplot(names(termFrequency), termFrequency, geom="bar", stat="identity") + coord_flip()
```


The word cloud option gives a colorful way of plotting the frequently occurring word.


```{r cloud,echo=FALSE,message=FALSE,warning=FALSE}
# Word cloud plot
tdmat = as.matrix(tdm)
v = sort(rowSums(tdmat), decreasing=TRUE)
d = data.frame(word=names(v), freq=v)
library(wordcloud)
set.seed(1)
wordcloud(d$word, d$freq, min.freq=200, random.color=TRUE,colors=rainbow(7))
```


#### Clustering of words

Getting a hierarchical clustering of the data will help understand the patterns in the email data.

```{r hclust,echo=FALSE,warning=FALSE,message=FALSE}
# Hierarchical Clustering
tdmat <- as.matrix(removeSparseTerms(tdm, 0.97))
distMatrix <- dist(scale(tdmat))
fit <- hclust(distMatrix, method="ward.D2")
plot(fit,labels=F)
```

This shows a clear pattern. Lets make boxes to delinate the 2 clusters.

```{r hclust1,echo=FALSE,warning=FALSE,message=FALSE}
plot(fit,labels=F)
rect.hclust(fit,k=2)
```

The pattern very clearly reveal 2 clusters. This is possibly the manifestation of the differences between the responsive and non-responsive emails.

If you have R version that is NOT "version 3.2.0", you may try the following vizualization of correlation plot
library(graph)
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.1, weighting = T)
These packages are not available for R version 3.2.0 that I have. So these plots are omitted from this analysis.

#### Models

Lets start building models to enable the prediction of an email being responsive.

Before that we need to join the cleaned and processed dtm data with our response variable, responsive. And also, we see that some of the column names are not legitimate R column names. So we would have to change the names of these columns. We use the make.names on the colnames to get the column names to legitimate R column names. Lets take a look at the structure of the first 10 rows of dat data frame.

```{r final_data,echo=FALSE,warning=FALSE,message=FALSE}

dat = as.data.frame(as.matrix(dtm))

dat$resp = as.factor(ifelse(emails$responsive==1,"yes","no"))

colnames(dat) = make.names(colnames(dat))

str(dat[1:10])

```


We now proceed to split the model into training anf test sets. As always, we use the caret package for this and do a 70/30 split.

```{r TrainTest,echo=FALSE,warning=FALSE,message=FALSE}
library(caret)
set.seed(1)
inTraining <- createDataPartition(dat$resp, p = .7, list = FALSE)
train <- dat[ inTraining,]
test  <- dat[-inTraining,]
#str(train)
```


##### Trees

Lets start with a simple tree model.

```{r tree,echo=FALSE,message=FALSE,warning=FALSE}
tree <- train(resp ~., data = train, method = "rpart")
tree
library(rattle)
fancyRpartPlot(tree$finalModel)
tree.pred = predict(tree,test)
confusionMatrix(test$resp,tree.pred)
accu_tree = confusionMatrix(test$resp, tree.pred)$overall[1]

print(paste0("The untuned Tree model gives an accuracy of ", as.character(round(accu_tree,4)*100)," %"))

```

Lets see if we can improve this model any further by using 10-fold CV

```{r tree1,echo=FALSE,message=FALSE,warning=FALSE}
fitControl <- trainControl(method ="cv",number = 10)
tree1 <- train(resp ~., data = train, method = "rpart", trControl = fitControl)
tree1
fancyRpartPlot(tree1$finalModel)
tree1.pred = predict(tree1,test)
confusionMatrix(test$resp,tree1.pred)
accu_tree1 = confusionMatrix(test$resp, tree1.pred)$overall[1]

print(paste0("The Tree model resampled with 10-fold CV gives an accuracy of ", as.character(round(accu_tree1,4)*100)," %"))

print(paste0("This tree1 model provides an improvement of ", as.character(round((accu_tree1/accu_tree - 1) * 100,2)) , " % over the tree model with default resampling, tree" ))

```


We enable Parallel Processing to shorten the computing times.

```{r pp,echo=FALSE,warning=FALSE,message=FALSE}
library(doParallel)
x = detectCores()
cl<-makeCluster(x)
registerDoParallel(cl)

print(paste0("Number of registered cores is ",x))

```

But the parallel processing may give slightly non-reproducable results.

Lets try random forest model.

```{r rf,echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
rf= train(x = train[,-789], y = train[,789], method = "rf",do.trace=F,
                  importance = T,allowParallel=T)
rf
rf.pred = predict(rf,test)
confusionMatrix(test$resp,rf.pred)
accu_rf = confusionMatrix(test$resp, rf.pred)$overall[1]
print(paste0("The untuned rf model gives an accuracy of ", as.character(round(accu_rf,4)*100)," %"))
```
  

Lets continue with RF, but use 10-fold CV as the resampling method instead of the default bootstrapping.

```{r rf1,echo=FALSE,message=FALSE,warning=FALSE}
set.seed(3000)
rf1= train(x = train[,-789], y = train[,789], method = "rf",do.trace=F,
                  importance = T,allowParallel=T,trControl = fitControl)
rf1
rf1.pred = predict(rf1,test)
confusionMatrix(test$resp,rf1.pred)
accu_rf1 = confusionMatrix(test$resp, rf1.pred)$overall[1]
print(paste0("The 10-fold cross validates rf1 model gives an accuracy of ", as.character(round(accu_rf1,4)*100)," %"))
```

Lets look at boosting models with 10-fold CV.

```{r gbm, echo=FALSE,warning=FALSE,message=FALSE}
set.seed(3000)
gbm<-train(x = train[,-789], y = train[,789], method = "gbm",trControl = fitControl)
gbm
gbm.pred = predict(gbm,test[,-789])
confusionMatrix(test$resp,gbm.pred)
accu_gbm = confusionMatrix(test$resp, gbm.pred)$overall[1]
print(paste0("The untuned gbm model gives an accuracy of ", as.character(round(accu_gbm,4)*100)," %"))
```

Lets try boosting with repeated (10 times) 10-fold CV

```{r gbm1, echo=FALSE,warning=FALSE,message=FALSE}
fitControl1 <- trainControl(method ="repeatedcv",number = 10,repeats=10)
set.seed(3000)
gbm1 <- train(x = train[,-789], y = train[,789], method = "gbm",trControl = fitControl1)
gbm1
gbm1.pred = predict(gbm1,test[,-789])
confusionMatrix(test$resp,gbm1.pred)
accu_gbm1 = confusionMatrix(test$resp, gbm1.pred)$overall[1]
print(paste0("The untuned gbm model gives an accuracy of ", as.character(round(accu_gbm1,4)*100)," %"))
```


Lets look at the 20 most important contributors to the gbm1 model prediction.

```{r gbm_imp,echo=FALSE,message=FALSE,warning=FALSE}

gbm1Imp <- varImp(gbm1, scale = FALSE)
plot(gbm1Imp, top = 20,main="20 most important predictor words in predicting 'responsive'ness of an email")

```

The plot makes sense in the context of the problem in hand. We see that the most important predictior words, in decreasing order of importance are:

- California
- price
- load
- capac (capacity)
- demand

which fits well within the context of energy company and demand manipulation.

Since we find that the word 'california' is the most important predictor word, it would be insightful to know which words are closely associated, in statistical terms, correlated with it. Lets look at the set of words which are correlated with the occurrence of the word 'california'. We specift the correlation threshold to be 0.7.

Lets repeat the process for the 5 most important predictor words. The correlation threshold for the second and other important predictor words are set to 0.6.

```{r cali,echo=FALSE,message=FALSE,warning=FALSE}

findAssocs(dtm, "california",corlimit=0.7)

findAssocs(dtm, "price",corlimit=0.6)

findAssocs(dtm, "load",corlimit=0.6)

findAssocs(dtm, "capac",corlimit=0.6)

findAssocs(dtm, "demand",corlimit=0.6)

```

Not surprisingly, we see that electr(icity), consum(er) and profit are most correlated with california. Its worth keeping in mind that the alphabets within brackets indicate the parts of words removed by stemming.

### Result

```{r result,echo=FALSE,message=FALSE,warning=FALSE}

result <- data.frame(model= character(6), accuracy= numeric(6))
result$model = c("tree","tree1","rf","rf1","gbm","gbm1")
result$accuracy = round(c(accu_tree,accu_tree1,accu_rf,accu_rf1,accu_gbm,accu_gbm1),4)*100

print(paste0("The least accurate prediction was ",min(result$accuracy)," %, given by ",result$model[which.min(result$accuracy)]," model"))

print(paste0("The most accurate prediction was ",max(result$accuracy)," %, given by ",result$model[which.max(result$accuracy)]," model"))

print(paste0("The most accurate model ",result$model[which.max(result$accuracy)]," gives an improvement of ", round((max(result$accuracy)/min(result$accuracy) - 1) * 100,3) , "% over the least accurate model of " ,result$model[which.min(result$accuracy)]))

print(paste0("With this model, we can expect close to ", max(result$accuracy) ,"% of correct predictions of responsiveness on a similar email datset that the model hasnt 'seen' yet."))

```

### Conclusion

We have used text analytics to decipher and evaluate a set of emails with respect to a response variable that characterizes the emails into 2 mutually exclusive categories.

In the process of setting up the model, we explored the various preprocessing options like
- converting the text to lower case
- removing punctuations
- definition and use of stop words
- introduction to stemming, types and its use in text analytics
- converting the data into a sparse matrix form
- numerous representations of frequently occurring words using ggplot and word cloud
- hierarchical clustering of the data to reveal clear patterns in the emails
- setting up of parallel processing
- CART models
- Random Forest models
- Boosting models
- important predictor words from the model
- correlations of most important predictors

This analysis forms part of whats called 'predictive coding'. In essence, predictive coding gets the input from a human, who reviews samples of documents and marks them according to the need of the task (responsive or benign in out context) and uses this human decision as its input to predict or generalize these decisions across a larger collection of documents. So this allows to atleast partly replace, if not predominantly, the expensive and tedious manual investigation of a large set of documents by computer programs. 

This is of great importance as recently, in April 2012, a state judge in Virginia issued the first state court ruling allowing the use of predictive coding in e-discovery in the case Global Aerospace, Inc. With such a ruling, its not difficult to see the optential and scope for predictive coding.

Footnote:
If the file gives any error while being converted to htlm using the 'Knit HTML' option in RStudio, the follwing code snippet can be used.
library(knitr)
knit2html("enron.Rmd")